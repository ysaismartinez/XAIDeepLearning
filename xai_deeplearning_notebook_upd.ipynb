{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ef59fcf",
      "metadata": {
        "id": "8ef59fcf"
      },
      "source": [
        "# Model Explainability in Computer Vision with Grad-CAM\n",
        "\n",
        "This notebook implements **GradCAM** and at least **two variants** on a small image classification task to analyze what regions a pretrained model attends to.  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6c1dc1",
      "metadata": {
        "id": "6a6c1dc1"
      },
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6809df64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6809df64",
        "outputId": "78bda54e-6c32-4036-92ad-c432676a22bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.5.5.tar.gz (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Collecting ttach (from grad-cam)\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from grad-cam) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from grad-cam) (1.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n",
            "Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.5-py3-none-any.whl size=44284 sha256=5cbacdc9cb34462ee2d2f1c16937590d1b802d8545303ac96993009d4567098b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/3b/09/2afc520f3d69bc26ae6bd87416759c820a3f7d05c1a077bbf6\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, grad-cam\n",
            "Successfully installed grad-cam-1.5.5 ttach-0.0.3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# If running in a new environment, uncomment and run the next cell to install dependencies.\n",
        "# There is more than one way to skin a cow, so you can this command using pip as option in a python command or just use the pip command below.\n",
        "# If your intent is just ro un this project, take the path of least resistance and just run it on Colab. Link provided in the README.\n",
        "%pip install torch torchvision timm grad-cam matplotlib opencv-python numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "201c930d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "201c930d",
        "outputId": "4ff3a0a9-daa3-4949-efe7-7501af02b954"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import os, random, json, time, math, pathlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, ScoreCAM, EigenCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc96d7a",
      "metadata": {
        "id": "acc96d7a"
      },
      "source": [
        "\n",
        "## 2) Choose a Task & Dataset\n",
        "\n",
        "We'll do **Recycling Sorting** (environmental sustainability): classify images as types of recyclable items (e.g., *plastic*, *paper*, *metal*).  \n",
        "\n",
        "**Option A (Recommended):** Use your own small dataset in an `ImageFolder` directory structure. Make sure you have your folders with the images or this project won't work. Alterntively used the images that I already have here in the folder. When I was going to submit the assignment I added the images locally. If this is the path you choose, no further action is required. I submitted this code to GitHub in a way that you just need to set up the project and run it:\n",
        "\n",
        "```\n",
        "data/recycling/\n",
        "  plastic/\n",
        "     img1.jpg\n",
        "     img2.jpg\n",
        "  paper/\n",
        "     img3.jpg\n",
        "  metal/\n",
        "     img4.jpg\n",
        "```\n",
        "\n",
        "Put at least **5–10 images total** across 2–3 classes (you can use your phone or royalty‑free images). The option B below is just to make sure that you have a fool proof way to run the project. If you don't have a folder or misspelled it, it will fall on a dummy array of data to run the project.\n",
        "\n",
        "**Option B (Fallback demo, just an alternative to make sure things run):** If you don't have a recycling folder, we'll automatically use **CIFAR‑10** and map categories to a pseudo‑recycling problem (not perfect, but it demonstrates the methods). It uses a small data structure and the logic is found on the else block below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e365aad6",
      "metadata": {
        "id": "e365aad6"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to anchor at the repo root (folder that contains a .git)\n",
        "def find_repo_root(start=Path.cwd()):\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \".git\").exists():\n",
        "            return p\n",
        "    return start  # fallback\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "DATA_DIR  = REPO_ROOT / \"data\" / \"recycling\"\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "common_tfms = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "def load_imagefolder_dataset(path: pathlib.Path):\n",
        "    ds = ImageFolder(str(path), transform=common_tfms)\n",
        "    classes = ds.classes\n",
        "    return ds, classes\n",
        "\n",
        "def load_cifar10_demo():\n",
        "    tfms = T.Compose([T.Resize((IMG_SIZE, IMG_SIZE)), T.ToTensor()])\n",
        "    ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfms)\n",
        "    # Below is the fall back data structure that I mentioned in the intro to this section.\n",
        "    class_map = {\n",
        "        0: \"airplane->metal\",\n",
        "        1: \"automobile->metal\",\n",
        "        2: \"bird->paper\",\n",
        "        3: \"cat->paper\",\n",
        "        4: \"deer->paper\",\n",
        "        5: \"dog->paper\",\n",
        "        6: \"frog->organic\",\n",
        "        7: \"horse->paper\",\n",
        "        8: \"ship->metal\",\n",
        "        9: \"truck->metal\",\n",
        "    }\n",
        "    return ds, class_map\n",
        "\n",
        "use_imagefolder = DATA_DIR.exists() and any(DATA_DIR.iterdir())\n",
        "if use_imagefolder:\n",
        "    dataset, class_names = load_imagefolder_dataset(DATA_DIR)\n",
        "    print(f\"Loaded custom dataset from {DATA_DIR}. Classes: {class_names}\")\n",
        "else:\n",
        "    dataset, class_map = load_cifar10_demo()\n",
        "    print(\"Using CIFAR-10 fallback demo (not a true recycling dataset).\")\n",
        "    print(\"Class mapping example:\", {k: class_map[k] for k in sorted(class_map)[:5]})\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3d45c3",
      "metadata": {
        "id": "1d3d45c3"
      },
      "source": [
        "## 3) Load a Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25988069",
      "metadata": {
        "id": "25988069"
      },
      "outputs": [],
      "source": [
        "\n",
        "# We'll use a robust ImageNet-pretrained model.\n",
        "model_name = \"resnet50\"\n",
        "model = timm.create_model(model_name, pretrained=True).to(device).eval()\n",
        "\n",
        "# Identify the final convolutional block for CAM\n",
        "# For ResNet50, this is layer4[-1]\n",
        "target_layers = [model.layer4[-1]]\n",
        "print(\"Model loaded:\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ee7b6e",
      "metadata": {
        "id": "87ee7b6e"
      },
      "source": [
        "## 4) Utilities for Visualization & Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16034bf4",
      "metadata": {
        "id": "16034bf4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tensor_to_rgb_img(t):\n",
        "    arr = t.detach().cpu().numpy()\n",
        "    if arr.ndim == 4:\n",
        "        arr = arr[0]\n",
        "    arr = np.transpose(arr, (1,2,0))\n",
        "    arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)\n",
        "    return arr\n",
        "\n",
        "def predict_topk(model, x, k=5):\n",
        "    with torch.no_grad():\n",
        "        logits = model(x.to(device))\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        topk = torch.topk(probs, k, dim=1)\n",
        "    return topk.indices[0].cpu().tolist(), topk.values[0].cpu().tolist()\n",
        "\n",
        "def show_image_with_title(img_tensor, title=\"\"):\n",
        "    rgb = tensor_to_rgb_img(img_tensor)\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(rgb)\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4821a525",
      "metadata": {
        "id": "4821a525"
      },
      "source": [
        "## 5) Grad-CAM and Variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40bd9a3",
      "metadata": {
        "id": "d40bd9a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define CAM methods\n",
        "cam_methods = {\n",
        "    \"GradCAM\":       GradCAM(model=model, target_layers=target_layers),\n",
        "    \"GradCAM++\":     GradCAMPlusPlus(model=model, target_layers=target_layers),\n",
        "    \"ScoreCAM\":      ScoreCAM(model=model, target_layers=target_layers),\n",
        "    \"EigenCAM\":      EigenCAM(model=model, target_layers=target_layers),\n",
        "}\n",
        "\n",
        "list(cam_methods.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa95a56",
      "metadata": {
        "id": "6fa95a56"
      },
      "source": [
        "## 6) Run on ≥5 Images and Generate Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c16f7458",
      "metadata": {
        "id": "c16f7458"
      },
      "outputs": [],
      "source": [
        "\n",
        "NUM_IMAGES = 5  # adjust if we want more. If you added 10 images, use 10.\n",
        "results = []\n",
        "\n",
        "for idx, (img, label) in enumerate(loader):\n",
        "    if idx >= NUM_IMAGES: break\n",
        "    img = img.to(device)\n",
        "    # Prediction\n",
        "    top_idxs, top_probs = predict_topk(model, img, k=3)\n",
        "    pred_info = list(zip(top_idxs, [float(p) for p in top_probs]))\n",
        "\n",
        "    # Build CAM targets (use predicted top-1 class for visualization)\n",
        "    targets = [ClassifierOutputTarget(top_idxs[0])]\n",
        "\n",
        "    rgb_img = tensor_to_rgb_img(img)\n",
        "    show_image_with_title(img, title=f\"Input Image {idx+1}\")\n",
        "\n",
        "    per_image = {\"index\": idx, \"pred_topk\": pred_info, \"cams\": {}}\n",
        "    for name, cam in cam_methods.items():\n",
        "        grayscale_cam = cam(input_tensor=img, targets=targets)[0]\n",
        "        vis = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "        per_image[\"cams\"][name] = grayscale_cam\n",
        "\n",
        "        plt.figure(figsize=(4,4))\n",
        "        plt.imshow(vis)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{name} — Image {idx+1}\")\n",
        "        plt.show()\n",
        "    results.append(per_image)\n",
        "\n",
        "print(\"Completed CAM visualizations for\", len(results), \"images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cdf2b93",
      "metadata": {
        "id": "5cdf2b93"
      },
      "source": [
        "\n",
        "## 7) Comparative Analysis (Write-up)\n",
        "\n",
        "For each image, compare **GradCAM**, **GradCAM++**, **ScoreCAM**, and **EigenCAM**:\n",
        "\n",
        "- **Localization & sharpness:** Which method produced the most focused attention on the true object?\n",
        "- **Stability:** Is one method more consistent across images?\n",
        "- **Failure modes:** Any method highlighting background or text instead of the object?\n",
        "- **Class sensitivity:** Did any method change drastically if you set the target to the ground-truth class vs. the model’s predicted class?\n",
        "\n",
        "Use the space below to add your observations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a49bc620",
      "metadata": {
        "id": "a49bc620"
      },
      "source": [
        "\n",
        "### Notes / Observations\n",
        "- Image 1: _your notes here_\n",
        "- Image 2: _your notes here_\n",
        "- Image 3: _your notes here_\n",
        "- Image 4: _your notes here_\n",
        "- Image 5: _your notes here_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2822982",
      "metadata": {
        "id": "e2822982"
      },
      "source": [
        "\n",
        "## 8) Reflection\n",
        "\n",
        "- **Visual cues the model attends to:** What consistent patterns did you observe? Did the model focus on shape, texture, backgrounds, or brand logos?\n",
        "- **Surprising or misleading behavior:** Document at least one example where the heatmap attention did not align with the object of interest.\n",
        "- **Why explainability matters in recycling:** In sustainability tasks, explainability builds trust that models aren’t keying off spurious correlations (e.g., studio background instead of material type). It also helps guide data collection (diversity of lighting, backgrounds, containers).\n",
        "\n",
        "_Add 1–2 paragraphs summarizing your reflections._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b77681",
      "metadata": {
        "id": "52b77681"
      },
      "source": [
        "\n",
        "## 9) Conclusion & Next Steps\n",
        "- **Summary:** Which CAM method do you find most reliable for this task and why?\n",
        "- **Limitations:** Small dataset size, potential dataset bias, coarse localization of CAMs.\n",
        "- **Future work:** Try more images, add proper recycling dataset (e.g., TrashNet/Kaggle Recycle), evaluate quantitative metrics (pointing game, deletion/insertion), and test other models (ViT).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
