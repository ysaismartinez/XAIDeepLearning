{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef59fcf",
   "metadata": {},
   "source": [
    "# Model Explainability in Computer Vision with Grad-CAM\n",
    "**Student:** _Your Name Here_  \n",
    "**Course:** _Course Name_  \n",
    "**Date:** 2025-10-03\n",
    "\n",
    "This notebook implements **GradCAM** and at least **two variants** on a small image classification task to analyze what regions a pretrained model attends to.  \n",
    "It follows the assignment instructions and rubric you provided.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c1dc1",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in a new environment, uncomment and run the next cell to install dependencies.\n",
    "# %pip install torch torchvision timm grad-cam matplotlib opencv-python numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, json, time, math, pathlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, ScoreCAM, EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc96d7a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Choose a Task & Dataset\n",
    "\n",
    "We'll do **Recycling Sorting** (environmental sustainability): classify images as types of recyclable items (e.g., *plastic*, *paper*, *metal*).  \n",
    "To make this portable and easy, the notebook supports **two options**:\n",
    "\n",
    "**Option A (Recommended):** Use your own small dataset in an `ImageFolder` directory structure:\n",
    "\n",
    "```\n",
    "data/recycling/\n",
    "  plastic/\n",
    "     img1.jpg\n",
    "     img2.jpg\n",
    "  paper/\n",
    "     img3.jpg\n",
    "  metal/\n",
    "     img4.jpg\n",
    "```\n",
    "\n",
    "Put at least **5–10 images total** across 2–3 classes (you can use your phone or royalty‑free images).\n",
    "\n",
    "**Option B (Fallback demo):** If you don't have a recycling folder, we'll automatically use **CIFAR‑10** and map categories to a pseudo‑recycling problem (not perfect, but it demonstrates the methods).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = pathlib.Path(\"data/recycling\")  # Change if your folder is elsewhere\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "common_tfms = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "def load_imagefolder_dataset(path: pathlib.Path):\n",
    "    ds = ImageFolder(str(path), transform=common_tfms)\n",
    "    classes = ds.classes\n",
    "    return ds, classes\n",
    "\n",
    "def load_cifar10_demo():\n",
    "    tfms = T.Compose([T.Resize((IMG_SIZE, IMG_SIZE)), T.ToTensor()])\n",
    "    ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfms)\n",
    "    # Map a few CIFAR10 classes into pseudo \"recycling\" buckets for demo\n",
    "    # (This is just to satisfy the pipeline if you don't have a custom folder.)\n",
    "    class_map = {\n",
    "        0: \"airplane->metal\",\n",
    "        1: \"automobile->metal\",\n",
    "        2: \"bird->paper\",\n",
    "        3: \"cat->paper\",\n",
    "        4: \"deer->paper\",\n",
    "        5: \"dog->paper\",\n",
    "        6: \"frog->organic\",\n",
    "        7: \"horse->paper\",\n",
    "        8: \"ship->metal\",\n",
    "        9: \"truck->metal\",\n",
    "    }\n",
    "    return ds, class_map\n",
    "\n",
    "use_imagefolder = DATA_DIR.exists() and any(DATA_DIR.iterdir())\n",
    "if use_imagefolder:\n",
    "    dataset, class_names = load_imagefolder_dataset(DATA_DIR)\n",
    "    print(f\"Loaded custom dataset from {DATA_DIR}. Classes: {class_names}\")\n",
    "else:\n",
    "    dataset, class_map = load_cifar10_demo()\n",
    "    print(\"Using CIFAR-10 fallback demo (not a true recycling dataset).\")\n",
    "    print(\"Class mapping example:\", {k: class_map[k] for k in sorted(class_map)[:5]})\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d45c3",
   "metadata": {},
   "source": [
    "## 3) Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25988069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll use a robust ImageNet-pretrained model.\n",
    "model_name = \"resnet50\"\n",
    "model = timm.create_model(model_name, pretrained=True).to(device).eval()\n",
    "\n",
    "# Identify the final convolutional block for CAM\n",
    "# For ResNet50, this is layer4[-1]\n",
    "target_layers = [model.layer4[-1]]\n",
    "print(\"Model loaded:\", model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee7b6e",
   "metadata": {},
   "source": [
    "## 4) Utilities for Visualization & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16034bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tensor_to_rgb_img(t):\n",
    "    arr = t.detach().cpu().numpy()\n",
    "    if arr.ndim == 4:\n",
    "        arr = arr[0]\n",
    "    arr = np.transpose(arr, (1,2,0))\n",
    "    arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)\n",
    "    return arr\n",
    "\n",
    "def predict_topk(model, x, k=5):\n",
    "    with torch.no_grad():\n",
    "        logits = model(x.to(device))\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        topk = torch.topk(probs, k, dim=1)\n",
    "    return topk.indices[0].cpu().tolist(), topk.values[0].cpu().tolist()\n",
    "\n",
    "def show_image_with_title(img_tensor, title=\"\"):\n",
    "    rgb = tensor_to_rgb_img(img_tensor)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821a525",
   "metadata": {},
   "source": [
    "## 5) Grad-CAM and Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40bd9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define CAM methods\n",
    "cam_methods = {\n",
    "    \"GradCAM\": GradCAM(model=model, target_layers=target_layers, use_cuda=(device=='cuda')),\n",
    "    \"GradCAM++\": GradCAMPlusPlus(model=model, target_layers=target_layers, use_cuda=(device=='cuda')),\n",
    "    \"ScoreCAM\": ScoreCAM(model=model, target_layers=target_layers, use_cuda=(device=='cuda')),\n",
    "    \"EigenCAM\": EigenCAM(model=model, target_layers=target_layers, use_cuda=(device=='cuda')),\n",
    "}\n",
    "\n",
    "list(cam_methods.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa95a56",
   "metadata": {},
   "source": [
    "## 6) Run on ≥5 Images and Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_IMAGES = 5  # adjust if you want more\n",
    "results = []\n",
    "\n",
    "for idx, (img, label) in enumerate(loader):\n",
    "    if idx >= NUM_IMAGES: break\n",
    "    img = img.to(device)\n",
    "    # Prediction\n",
    "    top_idxs, top_probs = predict_topk(model, img, k=3)\n",
    "    pred_info = list(zip(top_idxs, [float(p) for p in top_probs]))\n",
    "\n",
    "    # Build CAM targets (use predicted top-1 class for visualization)\n",
    "    targets = [ClassifierOutputTarget(top_idxs[0])]\n",
    "\n",
    "    rgb_img = tensor_to_rgb_img(img)\n",
    "    show_image_with_title(img, title=f\"Input Image {idx+1}\")\n",
    "\n",
    "    per_image = {\"index\": idx, \"pred_topk\": pred_info, \"cams\": {}}\n",
    "    for name, cam in cam_methods.items():\n",
    "        grayscale_cam = cam(input_tensor=img, targets=targets)[0]\n",
    "        vis = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "        per_image[\"cams\"][name] = grayscale_cam\n",
    "\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(vis)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{name} — Image {idx+1}\")\n",
    "        plt.show()\n",
    "    results.append(per_image)\n",
    "\n",
    "print(\"Completed CAM visualizations for\", len(results), \"images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf2b93",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Comparative Analysis (Write-up)\n",
    "\n",
    "For each image, compare **GradCAM**, **GradCAM++**, **ScoreCAM**, and **EigenCAM**:\n",
    "\n",
    "- **Localization & sharpness:** Which method produced the most focused attention on the true object?\n",
    "- **Stability:** Is one method more consistent across images?\n",
    "- **Failure modes:** Any method highlighting background or text instead of the object?\n",
    "- **Class sensitivity:** Did any method change drastically if you set the target to the ground-truth class vs. the model’s predicted class?\n",
    "\n",
    "Use the space below to add your observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bc620",
   "metadata": {},
   "source": [
    "\n",
    "### Notes / Observations\n",
    "- Image 1: _your notes here_\n",
    "- Image 2: _your notes here_\n",
    "- Image 3: _your notes here_\n",
    "- Image 4: _your notes here_\n",
    "- Image 5: _your notes here_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2822982",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Reflection\n",
    "\n",
    "- **Visual cues the model attends to:** What consistent patterns did you observe? Did the model focus on shape, texture, backgrounds, or brand logos?\n",
    "- **Surprising or misleading behavior:** Document at least one example where the heatmap attention did not align with the object of interest.\n",
    "- **Why explainability matters in recycling:** In sustainability tasks, explainability builds trust that models aren’t keying off spurious correlations (e.g., studio background instead of material type). It also helps guide data collection (diversity of lighting, backgrounds, containers).\n",
    "\n",
    "_Add 1–2 paragraphs summarizing your reflections._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b77681",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Conclusion & Next Steps\n",
    "- **Summary:** Which CAM method do you find most reliable for this task and why?\n",
    "- **Limitations:** Small dataset size, potential dataset bias, coarse localization of CAMs.\n",
    "- **Future work:** Try more images, add proper recycling dataset (e.g., TrashNet/Kaggle Recycle), evaluate quantitative metrics (pointing game, deletion/insertion), and test other models (ViT).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
